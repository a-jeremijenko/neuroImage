{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0914168-b345-4ab6-9082-3e6477159f93",
   "metadata": {},
   "source": [
    "# Part 1: Introduction and Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58664d3-2974-49ca-a34d-c8134f2a601d",
   "metadata": {},
   "source": [
    "## Image reconstruction from brain data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad6fee9-ef78-4665-8c0c-e455f25df661",
   "metadata": {},
   "source": [
    "### Motivation and background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b44a38-1e6a-4a8c-a856-580959938e42",
   "metadata": {},
   "source": [
    "This report will aim to explore different methods for retrieving visual information from brain data. This is an extremely novel and interesting task whose effective completion will be a significant milestone in our interpretation of the brain and its function. The key difficulties in reconstructing images from brain data are as follows:\n",
    "1. brain representations are not concrete, they are abstract. Our brain does not keep our visual imagery in a nice compact embedding in one part of the brain. Information is distributed, abstracted, and partial. The brain does not keep tabs on everything that happens around us, but rather efficiently stores the salient components of it and the ones that we are focused on. \n",
    "2. The resolution of our imaging is limited. Especially with functional fmri, we can only get down to a certain resolution which is still much bigger than the level of the neuron. This means we are far from having access to all of the information encoded in the brain. \n",
    "3. Processes are still being developed for handling brain data, especially across subjects. Every brain is unique and every brain contains a lot of noise. The problem of extracting useful and shared information from across brains is tricky and is far from being perfected. \n",
    "\n",
    "\n",
    "Being able to understand and interpret the brain is one of the greatest challenges posed to the human race, and one whose solution would be a vindication of our grasp of the natural world. The problem of extracting visual, sound or semantic information from the brain is one of the most fundamental first steps to achieving this understanding. This analysis here is my first step into the space, and has been extremely interesting and rewarding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7941fe44-78bb-40b0-badd-44e21d69fcbd",
   "metadata": {},
   "source": [
    "### Problem formulation\n",
    "The problem is as follows. \n",
    "\n",
    "\n",
    "\n",
    "Participants are shown a series of images (I, identical across participants). This information is encoded within the brain, and measured by us. Theoretically, everything that we see should be recoverable from within that encoding. Hence, the problem here is to take that encoding â†’ E and find a function f(E), that will reconstruct our original image at minimal loss. \n",
    "\n",
    "\n",
    "\n",
    "$\\displaystyle \\min L( E) \\ =\\ I$ - f(E)\n",
    "\n",
    "\n",
    "\n",
    "Where f is our neural network. There are further preprocessing functions (p(E)) we could apply to the data to change it into a lower dimensional form, but this is the primary construction. \n",
    "\n",
    "\n",
    "A solution would be constituted by finding an architecture which can recreate the original images to the degree that they are recognisable for what they originally where. Complete image reconstruction will be extremely hard to achieve, getting to a recognizable standard is reasonable objective. \n",
    "\n",
    "### Method and justification\n",
    "\n",
    "The method and reasons will be expounded throughout the body of this notebook, here we provide a brief overview for the tricks and methods we employ:\n",
    "\n",
    "First, we employ dimensionality reduction. The methods commonly used to achieve this are PCA, and shared response modeling (SRM). PCA is familier, SRM is a more domain specific approach. Further, SRM allows us to align the brain spaces accross subjects, which is extremely useful. \n",
    "\n",
    "\n",
    "\n",
    "In SRM we minimize the following optimization problem:\n",
    "\n",
    "\n",
    "\n",
    "$\\displaystyle \\min I_{i} \\ =\\ w_{i} \\ \\times \\ E$\n",
    "\n",
    "\n",
    "\n",
    "Where E is an embedding which is shared across all subjects, w is a weight matrix specific to each subject and I is the original image. Essentially, we have to find weights and an embedding which most closely reconstructs the original brain space of each subject. This leaves us with a set of weights which we can use to transform each subjects data into a common space. This common space is typically of much smaller dimensionality (as decided by the researcher). \n",
    "\n",
    "\n",
    "\n",
    "In this report we will be using both methods and seeing how they perform. SRM is generally preffered, however, because it allows us to align brains across participants, which is an extremly useful thing. It effectively triples (in this case atleast and usually more) the size of our dataset. \n",
    "\n",
    "\n",
    "Further, we will also attempt to use GANs to reconstruct more realistic looking images. The problem with direct reconstruction is that we often end up with extremely messy, soupy looking images. A GAN can be used as a generator which searches for the nearest 'real looking neighbor' which produces an embedding similar to the one induced by our brains. To achieve this, theoretically, we train a neural network to take images into the embedding space which we have found using PCA or an SRM. Then we use the GAN to search for an image which most closely encodes to become the encoding that was generated by the brain. \n",
    "\n",
    "\n",
    "In this report we have had mixed success with this. It is a tricky task and very finnicky. Our preliminary results will be shared but this is certainly a task that will require more work to perfect. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56964192-49d1-49c2-b9e2-4462cf22f9a0",
   "metadata": {},
   "source": [
    "### Evaluation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbfa580-9c3b-4ca5-acb8-a387a615b54f",
   "metadata": {},
   "source": [
    "Once again, evaluation will be performed throughout the body of the notebook. Given the nature of this task, primarily evaluation will be qualitative. An Image is either reconstructed to some degree or its just a mess. \n",
    "\n",
    "More concretely, the metrics that we will use to optimize our functions are\n",
    "1. MSE (pixel by pixel), which is the primary optimization metric\n",
    "2. We will also attempt to employ a pretrained vision net to extract image embeddings and use those to evaluate the similarity of our images (Using MSE on the embeddings. Embeddings are extracted by feeding the recreated image and the original image through the vision net and seeing how the encoding looks like in the second to last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5900eec3-190c-43d6-b220-8fe555086249",
   "metadata": {},
   "source": [
    "### Results and insights will be shared throughout the report. \n",
    "A key insight is that this problem is really hard. I used largely preprocessed data, but to do this really well, it is probably necessary to have a handle on the whole procedure - i.e. very precisely handle the fmri data so that absolutely no valuable information is lost. We achieved recreation of color and some basic shape, however we are still a far cry from effective image recreation. With more time, I believe these methods can get there, however, much more optimization will be required. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bd22b9-af8f-412f-acc7-45f14f811a0b",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c19035-4f48-49f7-afb0-f5ccfa246771",
   "metadata": {},
   "source": [
    "In this section of the report, we download the data and make sure that it carries information about the images. In this way we can prove that we have imported the data correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7007d52-7ca7-466f-b8dc-ab840628918f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import brainiak.funcalign.srm\n",
    "import matplotlib.pyplot as plt\n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57226b38-1e36-4e18-9009-4994de009c87",
   "metadata": {},
   "source": [
    "After downloading our data from the OpenNeuro, we start with a simple classifier to test that there are no problems with the data. Here we only test on one subject because the brains have not yet been aligned. See make_data.py to see how the data is downloaded and preprocessed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac910eb-8b2d-4146-bd37-a96d137d1ab4",
   "metadata": {},
   "source": [
    "Repetitively taking a subsample of 2 images (5 ims each class so 10 data points) to classify on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dea41546-999d-424f-9bf6-56efb5842544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:04<00:00, 2303.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "sub_01_labels = np.load(\"np_data/sub_01_labels_train.npy\")\n",
    "sub_01_fmri = np.load(\"np_data/sub_01_fmri_train.npy\")\n",
    "sub_01_images = np.load(\"np_data/sub_01_images_train.npy\")\n",
    "\n",
    "# PCA on the fmri to help the classifier\n",
    "pca = PCA(n_components=300)\n",
    "decomposed_fmri = pca.fit_transform(sub_01_fmri)\n",
    "accuracies = []\n",
    "\n",
    "# Define the classifier\n",
    "clf = svm.SVC(gamma = 'auto')\n",
    "\n",
    "for i in tqdm(range(10000)):\n",
    "    ss = np.random.randint(1, 1200, size = 2) # taking a subsample of 2 categories\n",
    "    if ss[0] == ss[1]: # so we don't sample twice the same number\n",
    "        ss[1] = ss[0]+1\n",
    "    ss_coords = np.in1d(sub_01_labels, ss).nonzero()[0] # getting coords for all occurences of the class\n",
    "    shuffle = np.random.permutation(len(ss_coords)) # shuffle\n",
    "    labels_ss = (sub_01_labels[ss_coords])\n",
    "    fmri_ss = (decomposed_fmri[ss_coords])\n",
    "\n",
    "    # keeping the training and test data balanced\n",
    "    mask = np.hstack([np.random.choice(np.where(labels_ss == _class)[0], 4, replace=False)\n",
    "                      for _class in np.unique(labels_ss)])\n",
    "    \n",
    "    # Extract \n",
    "    selected = np.where(~np.in1d(np.arange(len(labels_ss)), mask))[0]\n",
    "\n",
    "    train_x, train_y = fmri_ss[mask], labels_ss[mask]\n",
    "    test_x, test_y = fmri_ss[selected], labels_ss[selected]\n",
    "\n",
    "    clf.fit(train_x, train_y)\n",
    "    accuracies.extend((clf.predict(test_x) == test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "423fca54-a280-462f-afcc-b14446860cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yielding us 0.56005 in a binary classification task\n"
     ]
    }
   ],
   "source": [
    "print(f\"yielding us {sum(accuracies)/len(accuracies)} in a binary classification task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3f9d95-35e6-4dcc-843f-8f360f74dfae",
   "metadata": {},
   "source": [
    "slightly above 50% accuracy. Not great, but not too bad considering we are only training on 8 datapoints per instance and are using a crude model. However, this is sufficient evidence to prove that the data is carrying information about the images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54f35d3-b03a-4700-a23e-51ddcea9b6f8",
   "metadata": {},
   "source": [
    "They have two versions of the data. Downloading the second version to see if it carries more info. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "961ad6a5-189e-421a-b134-1ccc74ef69dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:04<00:00, 2437.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "sub_01_labels = np.load(\"np_data_v2/sub_01_labels.npy\")\n",
    "sub_01_fmri = np.load(\"np_data_v2/sub_01_fmri.npy\")\n",
    "sub_01_images = np.load(\"np_data_v2/sub_01_images.npy\")\n",
    "\n",
    "# PCA on the fmri to help the classifier\n",
    "pca = PCA(n_components=300)\n",
    "decomposed_fmri = pca.fit_transform(sub_01_fmri)\n",
    "accuracies = []\n",
    "\n",
    "# Define the classifier\n",
    "clf = svm.SVC(gamma = 'auto')\n",
    "\n",
    "for i in tqdm(range(10000)):\n",
    "    ss = np.random.randint(1, 1200, size = 2) # taking a subsample of 2 categories\n",
    "    if ss[0] == ss[1]: # so we don't sample twice the same number\n",
    "        ss[1] = ss[0]+1\n",
    "    ss_coords = np.in1d(sub_01_labels, ss).nonzero()[0] # getting coords for all occurences of the class\n",
    "    shuffle = np.random.permutation(len(ss_coords)) # shuffle\n",
    "    labels_ss = (sub_01_labels[ss_coords])\n",
    "    fmri_ss = (decomposed_fmri[ss_coords])\n",
    "\n",
    "    # keeping the training and test data balanced\n",
    "    mask = np.hstack([np.random.choice(np.where(labels_ss == _class)[0], 4, replace=False)\n",
    "                      for _class in np.unique(labels_ss)])\n",
    "    \n",
    "    # Extract \n",
    "    selected = np.where(~np.in1d(np.arange(len(labels_ss)), mask))[0]\n",
    "\n",
    "    train_x, train_y = fmri_ss[mask], labels_ss[mask]\n",
    "    test_x, test_y = fmri_ss[selected], labels_ss[selected]\n",
    "\n",
    "    clf.fit(train_x, train_y)\n",
    "    accuracies.extend((clf.predict(test_x) == test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd8f2495-868e-4be9-b34c-dd0814d0655e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yielding us 0.53865 in a binary classification task\n"
     ]
    }
   ],
   "source": [
    "print(f\"yielding us {sum(accuracies)/len(accuracies)} in a binary classification task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e61f390-6ad5-4c75-a09c-ee131f6d68ad",
   "metadata": {},
   "source": [
    "53% classification accuracy vs 56%. Seems like the first version performs better, we will use the first version of the data in this report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17f8b23-c047-47f9-9b29-e36c17778988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
